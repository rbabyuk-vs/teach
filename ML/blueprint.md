## Blueprint: Основи Навчання З Учителем (Supervised Learning)

### 1. Визначення та Базовий Набір (The Basic Setup)

**Навчання з учителем (Supervised Learning)** — це традиційний підхід до машинного навчання, де модель навчається на основі вхідних даних, яким відповідає певний бажаний вихід або мітка (**ground truth**).

Основна мета полягає в тому, щоб модель навчилася давати прогноз, який максимально відповідає істинним міткам.

| Компонент | Опис згідно з Джерелами |
| :--- | :--- |
| **Вхід (Input)** | Дані, які надходять до моделі, можуть бути зображенням, текстом, аудіо, відео, або структурованими даними (наприклад, електронні таблиці). |
| **Вихід (Output)** | Бажаний результат. Це може бути **класифікація** (наприклад, 0 або 1) або **регресія** (оцінка числового значення, наприклад, віку). |
| **Модель (Model)** | Складається з **Архітектури** (схеми/скелета) та **Параметрів** (ваг), які підлягають тренуванню. |
| **Функція Втрат (Loss Function)** | Надає зворотний зв'язок моделі, порівнюючи прогноз із істинною міткою (ground truth) і розраховуючи "штраф" за неправильність. Дизайн функції втрат є мистецтвом. |
| **Навчання (Training)** | Використовує **оптимізацію градієнтного спуску (Gradient Descent)**. Це процес, коли параметри (ваги) моделі постійно коригуються, щоб мінімізувати значення функції втрат. |
| **Ємність (Capacity)** | Глибина та складність мережі. Більш глибока мережа має більшу ємність. Ємність повинна відповідати кількості та різноманітності даних, щоб уникнути перенавчання (overfit). |
| **Ознаки (Features)** | У глибокому навчанні (Deep Learning) модель автоматично вивчає ознаки (Feature Learning), на відміну від ручного створення ознак (Feature Engineering). Перші шари мережі кодують низькорівневі ознаки (наприклад, краї), а глибші шари — високорівневі (наприклад, очі чи ніс). |

---

### 2. Приклади Проектів Навчання З Учителем

#### 2.1. Класифікація "День або Ніч" (Day and Night Classification)

Це класичний приклад бінарної класифікації.

| Аспект | Деталі |
| :--- | :--- |
| **Завдання** | Класифікувати зображення як "День" або "Ніч". |
| **Вхід / Роздільна Здатність** | Зображення (наприклад, 64x64x3, де 3 — канали RGB). Необхідно вибрати достатню роздільну здатність, щоб зберегти важливу інформацію (наприклад, годинник), але не надто високу, щоб не збільшувати обчислювальні витрати. Колір (3 канали) має значення для цього завдання. |
| **Вихід / Активація** | Бінарне число (0 або 1). Остання функція активації — **Sigmoid** (стискає вихід між 0 і 1, імітуючи ймовірність). |
| **Функція Втрат** | Логістична втрата (Logistic Loss), також відома як **Бінарна крос-ентропія (Binary Cross-entropy loss)**. |
| **Критичні моменти** | Необхідно чітко визначити завдання (наприклад, лише для однієї локації чи для всього світу). Складні випадки включають знімки в приміщенні, світанок/сутінки, або хмарну погоду. |

#### 2.2. Детектування Ключового Слова (Trigger Word Detection)

Цей проект є прикладом роботи з послідовними даними (sequential data), наприклад, аудіо.

| Аспект | Деталі |
| :--- | :--- |
| **Завдання** | Виявити певне ключове слово (наприклад, "activate") у 10-секундному аудіокліпі. |
| **Стратегія Збору Даних** | Створення навчального набору шляхом використання трьох баз даних: позитивні слова, негативні слова та фоновий шум. Використовується скрипт для синтетичного створення кліпів шляхом випадкового додавання слів та фонового шуму. |
| **Стратегія Маркування (Labeling)** | Замість маркування всього кліпу як 0 або 1, використовується **послідовне маркування** (sequential labeling). Одиниці проставляються у вихідній послідовності тільки тоді, коли звучить позитивне слово. Цей підхід є ефективнішим і швидшим для навчання моделі. |
| **Вихід / Активація** | Активація **Sigmoid** використовується послідовно (в кожен часовий крок). |
| **Функція Втрат** | Бінарна крос-ентропія, що застосовується послідовно. |

#### 2.3. Верифікація Обличчя (Face Verification)

Цей приклад демонструє, як навчити модель створювати змістовні векторні представлення (embeddings).

| Аспект | Деталі |
| :--- | :--- |
| **Завдання** | Порівняти два зображення обличчя і визначити, чи це та сама особа. |
| **Проблема піксельного порівняння** | Пряме піксельне порівняння двох зображень неефективне через зміни в освітленні, фоні, відстані до камери, окулярах, бороді чи віці. |
| **Рішення** | Використання **Енкодера (Encoder Network)** — глибокої нейронної мережі, яка перетворює зображення на **вектор (embedding)** фіксованої розмірності (наприклад, 128 вимірів). Цей вектор повинен інкапсулювати змістовні ознаки обличчя. |
| **Тренувальна Стратегія** | Модель тренується на **тріплетах** (трійках) зображень:<ul><li>**Анкер (Anchor, A):** Базове зображення.</li><li>**Позитив (Positive, P):** Зображення тієї ж особи, що й Анкер.</li><li>**Негатив (Negative, N):** Зображення іншої особи.</li></ul> |
| **Функція Втрат** | **Втрата Тріплетом (Triplet Loss)**. Мета: **мінімізувати** відстань між векторами A і P та **максимізувати** відстань між векторами A і N. |
| **Застосування** | Отримані вектори дозволяють легко реалізувати **Ідентифікацію обличчя** (пошук найближчого вектора в базі даних, наприклад, через K-найближчих сусідів) або **Кластеризацію обличчя** (групування зображень однієї особи, наприклад, через алгоритм K-Means). |

***

### 3. Зв'язок з Іншими Типами Навчання

Хоча навчання з учителем вимагає явних міток, існують суміжні підходи:

1.  **Навчання з самоконтролем (Self-Supervised Learning, SSL):** Модель створює власні мітки на основі самих даних, використовуючи природні закономірності (наприклад, обертання зображень або передбачення наступного слова/токена). Це дозволяє тренувати потужні енкодери на мільярдах немаркованих зображень.
2.  **Навчання з неповним учителем (Weakly Supervised Learning):** Використовує мітки, які виникають природним чином у даних, наприклад, підписи (caption) до зображень в Інтернеті (наприклад, на Instagram). Це дозволяє зв'язувати різні модальності (текст, зображення, аудіо) у спільному векторному просторі.